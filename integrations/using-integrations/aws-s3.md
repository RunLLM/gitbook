# AWS S3

{% hint style="info" %}
Before starting, make sure you've added your S3 bucket as an integration to Aqueduct (see [adding-an-integration](../adding-an-integration/ "mention")).
{% endhint %}

First, we're going to get a connection to your S3 bucket by calling `.integration()` on the Aqueduct Client.&#x20;

```python
my_bucket = client.integration('s3/my_data_bucket')
```

### Reading from AWS S3

Once we've loaded a connection to our S3 bucket, we can access data by using the `.file()` method. `.file()` requires the following arguments:

* `filepaths`: One or more paths from which files should be read. You can also specify a directory
  name, in which case we will perform a prefix search. When multiple files are read, we concatenate them
  into a single DataFrame.
* `format`: The expected table format when reading from `filepaths`. We currently support `CSV`, `Parquet`, or `JSON`.

See below for example usage:

```python
customers = my_bucket.file(filepaths='data/sales/customers.csv', format='CSV')

# Since the `filepaths` contains more than one path, we concatenate the results into a single DataFrame.
cars = my_bucket.file(filepaths=['data/cars/mazda.json', 'data/cars/honda.json'], format='JSON')

# Since we specified a directory name, a prefix search is done to retrieve all files that match the search.
cars = my_bucket.file(filepaths='data/cars/', format='JSON')
```

This returns what we call an [**Artifact**](../../artifacts.md) **** in Aqueduct terminology. An artifact is simply just a wrapper around some data that we'll track a part of your workflow. You can call [Operators](../../operators.md) on your artifacts -- for more on this, see the guide on [Operators](../../operators.md) or [Workflows](../../workflows/).

{% hint style="info" %}
Note that Aqueduct currently only supports loading tabular data either in CSV files, JSON blobs, or Parquet files from AWS S3. Other file formats aren't yet supported.
{% endhint %}

{% hint style="warning" %}
Aqueduct does not eagerly validate whether or not your file exists. In order to ensure that your file exists, call `.get()` on the resulting Artifact. If the file does not exist or cannot be loaded, you will be notified immediately.
{% endhint %}

### Writing to AWS S3

In order to save an aritfact to a database, you can call `.save()` on the artifact. `.save()` expects information about which system to write the artifact to and what configuratin parameters are necessary. This configuration can be generated by calling `.config` on your integration object. For AWS S3, the following parameters are required:

* `filepath`: The path in the S3 bucket to which this file should be written.
* `format`: The format to which we convert the DataFrame before writing to `filepath`.
We currently support `CSV`, `Parquet`, or `JSON`.
